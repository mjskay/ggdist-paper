\documentclass[journal]{vgtc}                     % final (journal style)
%\documentclass[journal,hideappendix]{vgtc}        % final (journal style) without appendices
%\documentclass[review,journal]{vgtc}              % review (journal style)
%\documentclass[review,journal,hideappendix]{vgtc} % review (journal style)
%\documentclass[widereview]{vgtc}                  % wide-spaced review
%\documentclass[preprint,journal]{vgtc}            % preprint (journal style)


%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication in an open access repository,
%% and the final version doesn't use a specific qualifier.

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please use one of the ``review'' options and replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% In preprint mode you may define your own headline. If not, the default IEEE copyright message will appear in preprint mode.
%\preprinttext{To appear in IEEE Transactions on Visualization and Computer Graphics.}

%% In preprint mode, this adds a link to the version of the paper on IEEEXplore
%% Uncomment this line when you produce a preprint version of the article 
%% after the article receives a DOI for the paper from IEEE
%\ieeedoi{xx.xxxx/TVCG.201x.xxxxxxx}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% please declare the paper type of your paper to help reviewers, only shown in review mode
%% choices:
%% * algorithm/technique
%% * application/design study
%% * evaluation
%% * system
%% * theory/model
\vgtcpapertype{please specify}

%% Paper title.
\title{ggdist: Visualizations of Distributions and\\Uncertainty in the Grammar of Graphics}

%% Author ORCID IDs should be specified using \authororcid like below inside
%% of the \author command. ORCID IDs can be registered at https://orcid.org/.
%% Include only the 16-digit dashed ID.
\author{%
  \authororcid{Matthew Kay}{0000-0001-9446-0419}
}

\authorfooter{
  %% insert punctuation at end of each item
  \item
  	Matthew Kay is with Northwestern University.
  	E-mail: mjskay@northwestern.edu
}

%% Abstract section.
\abstract{%
  \lipsum[1] % filler text. Replace with your abstract.
  %
  %% We recommend that you link to your supplemental material here in the abstract, as well
  %% as in the Supplemental Materials section at the end.
  A free copy of this paper and all supplemental materials are available at \url{https://OSF.IO/2NBSG}.
}

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{Uncertainty visualization, distributions, grammar of graphics}

%% A teaser figure can be included as follows
\teaser{
  \centering
  \includegraphics[width=\linewidth]{CypressView}
  \caption{%
  	In the Clouds: Vancouver from Cypress Mountain.
  	Note that the teaser may not be wider than the abstract block.%
  }
  \label{fig:teaser}
}

%% Uncomment below to disable the manuscript note
%\renewcommand{\manuscriptnotetxt}{}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
%\nocopyrightspace


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% LOAD PACKAGES %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Tell graphicx where to find files for figures when calling \includegraphics.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% \includegraphics{diamondrule} is completely sufficient.
\graphicspath{{figs/}{figures/}{pictures/}{images/}{./}} % where to search for the images

%% Only used in the template examples. You can remove these lines.
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}                  % only used for the table example
\usepackage{lipsum}                    % used to generate placeholder text
\usepackage{mwe}         % used to generate placeholder figures
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{varwidth}

%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.
\usepackage{mathptmx}                  % use matching math font

\newenvironment{centerverbatim}{%
  \hfill\break
  \small
  \centering
  \varwidth{\linewidth}%
  \verbatim
}{%
  \endverbatim
  \endvarwidth
  \par
  \hfill\break
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.
%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}

\maketitle


%% \section{Introduction} %for journal use above \firstsection{..} instead

Uncertainty visualization, generally speaking, has only basic support in existing implementations of the grammar of graphics. Popular implementations like \textit{ggplot2}, \textit{Vega-lite}, and \textit{Observable Plot} typically provide versions of error bars (for points), uncertainty bands (for lines), boxplots, and density plots. Research in uncertainty visualization has long revealed problems with all of these representations and proposed a variety of alternative uncertainty representations to replace them. Examples include quantile dotplots, gradient error bars, gradient plots, gradient bands, and eye plots. Yet, most of these alternative representations are---simply put---painful to convince existing grammar of graphics implementations to produce.

\textit{ggdist} is an attempt to rectify this situation. It started under the guise of \textit{tidybayes}---an R package I wrote for post-processing Bayesian models for use with \textit{ggplot2}---in about 2016. I published \textit{tidybayes} to CRAN (the Comprehensive R Archive Network) in 2018, and it slowly gained some use in the Bayesian statistics community. However, the package had two complementary, but not always perfectly aligned, use cases: post-processing Bayesian model output for visualization, and uncertainty visualization in the grammar of graphics. The latter is bigger than \textit{just} Bayesian statistics: everyone needs to visualize uncertainty! And, contrary to popular opinion---as I'll demonstrate later---Bayesian and frequentist uncertainty visualization can in fact be done within the same framework. Recognizing this broader need, in 2020 I spun off the uncertainty visualization components of \textit{tidybayes} into a new package, \textit{ggdist}, and published it to CRAN. Since then it has steadily grown in use inside and outside the Bayesian statistics community in R, and now averages about 14,000 downloads per month (best described as ``modest'' for an R package).

Over the course of its continuing development, I have learned a lot about how to integrate uncertainty visualization into the grammar of graphics, and developed a formal way of describing that integration (more on that later). In the spirit of recent retrospectives on visualization system design~\textbf{CITE}, I'd like to distill down some of what I've learned here.\footnote{And in the spirit of this being a retrospective, I'm keeping the tone informal. I think that's more honest; and besides, after two years of a pandemic, I at least need a break from stilted academic writing. I hope you do too! If not, whatever.} Ultimately, what would be nice to have---and what I hope to demonstrate \textit{ggdist} to be---is a coherent extension to the grammar of graphics that makes it easy to create the various proposed alternative uncertainty visualizations from the literature, \textit{and more}. By  ``and more'' I mean: not only should we be able to express a variety of uncertainty visualizations through a single coherent framework, but that framework should be complete enough that someone else can wander along and express new uncertainty visualization types I've never thought of before (emphasis on \textit{I}---I think a formalism and its corresponding \textsc{api} truly shows its power when people make things with it that its creator has never thought of before). I'll do this by grounding the design of \textit{ggdist} in a formal representation of uncertainty through mappings of functions of distributions onto visual channels or \textit{aesthetics}. As time is a straight arrow, I can't provide direct evidence that there are visualization types I've never seen before that can be created with \textit{ggdist}, but I will describe instances in which I have encountered uncertainty visualizations in the literature and subsequently realized they were straightforward to express in \textit{ggdist}.

Ultimately, given the huge expressive power of the grammar of graphics and the popularity of tools built on it, I hope a catalog of my experiences here might provide a catalyst for improved implementations of uncertainty visualization to flourish in existing grammar of graphics ecosystems. Let's give it a try.

\section{Setting the stage}

\subsection{A simplified notation for the grammar of graphics}

To be able to talk more generally than a specific grammar of graphics implementation, we'll at least need a formal way of writing down visualization specifications separated from a particular implementation. I'll adopt here a notation that I've found works pretty well when I teach the grammar of graphics to undergraduates. I've used a variation on this notation when teaching \textit{ggplot2}, \textit{Vega-lite}, \textit{Altair}, and \textit{Tableau} and found students pick it up easily, which is at least some evidence it might be easy for folks to understand. The core notation describes a visualization in terms of its \textit{data variables}, \textit{aesthetic mappings}, and \textit{geometries}. We create a scatterplot, for example, by mapping one variable onto the \textit{x} aesthetic and another onto the \textit{y} aesthetic:

\hfill\break
\noindent
  \begin{minipage}{.5\columnwidth}
    \begin{align*}
\mathit{weight} &\rightarrow x\\
\mathit{mpg} &\rightarrow y\\
\textsc{geom} &= \mathit{point}
\end{align*}
  \end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/2-mpg_v_weight.pdf}
  \end{minipage}


This notation sets up two \textit{aesthetic mappings}:\footnote{Or in \textit{Vega-lite} parlance, \textit{encodings}} a mapping from the \textit{weight} data variable onto the \textit{x} aesthetic, and a mapping from the \textit{mpg} data variable onto the \textit{y} aesthetic. It then employs a \textit{point} geometry\footnote{\textit{Vega-lite}-ese: mark} for display. I like this notation because it emphasizes that we are creating \textit{functions} from data space to aesthetic (display) space; in grammar of graphics parlance these are \textit{scale} functions which can themselves be specified (e.g., to set up log scales, to pick how colors are assigned to data values, etc). Other notations obscure this key insight into the structure of the grammar of graphics by, e.g., placing the aesthetic first and ``assigning'' data variables to it, which gives an incorrect intuition in my view. In fact, let's see that now: here is a translation of the above into \textit{ggplot2} code, assuming \texttt{cars} is a data frame with \texttt{weight} and \texttt{mpg} columns:

\begin{centerverbatim}
ggplot(cars) +
  aes(
    x = weight,
    y = mpg
  ) +
  geom_point()
\end{centerverbatim}

Or in \textit{Vega-lite}:\footnote{I use the Vega-lite \textsc{api} instead of its \textsc{json} form, as \textsc{json} is a horrifying mess of visual noise that no sane human should want to read or write. The Vega-lite \textsc{api} is a notable improvement, though without R's facility for capturing and re-writing abstract syntax trees, it doesn't quite reach the succinctness of \textit{ggplot2}. This seems to be a fundamental limitation when writing domain-specific languages in JavaScript, and the \textit{Vega-lite} authors have done an excellent job with the language they've been given.}

\begin{centerverbatim}
vl.data("path/to/cars.json")
  .encode(
    vl.x().fieldQ("weight"),
    vl.y().fieldQ("mpg")
  )
  .markPoint()
\end{centerverbatim}

This shows the close correspondence between the abstract notation above and the particulars of code in actual grammar of graphics implementations. Throughout the rest of this paper, I'll stick to the abstract notation and corresponding \textit{ggplot2} + \textit{ggdist} code for the particulars.

\subsection{Uncertainty visualization in the grammar of graphics, as she is spoke}

Speaking of existing implementations of the grammar of graphics, how do they implement uncertainty visualization? Rudimentarily, I think.

One natural approach to uncertainty visualization is to assume a Gaussian approximation: to represent all estimates and their uncertainty as a mean and standard deviation. This makes the specification problem easy: instead of mapping a single value onto the \textit{x} aesthetic, say, we map a point estimate onto \textit{x} and provide an aesthetic for its standard deviation; call it $x_\textsc{sd}$. Say we had estimated a variable $a$ and had quantified its standard error (i.e. the standard deviation of its sampling distribution) as $\sigma_a$, we might plot a point with an error bar using a \textit{pointinterval} geometry as follows, yielding a 95\% interval calculated from a Normal distribution with mean $a$ and standard deviation $\sigma_a$:

\noindent
\begin{minipage}{.5\columnwidth}
\begin{align*}
a &\rightarrow x\\
\sigma_a &\rightarrow x_\textsc{sd}\\
\textsc{geom} &= \mathit{pointinterval}
\end{align*}
  \end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/2-mean_sd_interval.pdf}
  \end{minipage}

This is one approach taken by \textit{Vega-lite}: it provides an \texttt{errorbar}\footnote{\label{foot:errorbar}\texttt{errorbar} is, in my view, a misnomer, as is \texttt{xError}: fundamentally, the mark calculates a Gaussian interval, which might be for a distribution being used to represent error in an estimate, but might not. \textit{Error} is not the generic notion at play; a standard deviation is; and the generic mark is an interval, not an error bar.} mark (analogous to \textit{pointinterval}) and an \texttt{xError}  channel (analogous to $x_\textsc{sd}$). I'll refer to this notation as the $\{x, x_\textsc{sd}\}$ approach.

The problem, fundamentally, is that not all uncertainty is well-represented by a Gaussian distribution. Consider uncertainty in a proportion (bounded at 0 and 1, thus as estimates approach the boundary the interval becomes asymmetric---yet Gaussian intervals must be symmetric) or uncertainty in a variance parameter (bounded below at 0). Or, consider the ubiquitous Student-\textit{t} confidence interval: as the degrees of freedom go to $\infty$, a Student-\textit{t} distribution is well approximated by the Normal, but with low degrees of freedom (incidentally common in small-\textit{n} studies---like at \textsc{vis}), the tails of the distribution become fatter, and the Normal distribution is a poor approximation. Thus, a more general approach is needed.

The obvious alternative, at least for interval representations, is to simply specify the endpoints of the intervals; e.g. for a 95\% Gaussian interval:

\noindent
\begin{minipage}{.5\columnwidth}
\begin{align*}
a &\rightarrow x\\
a - 1.96 \cdot \sigma_a &\rightarrow x_\textsc{min}\\
a + 1.96 \cdot \sigma_a &\rightarrow x_\textsc{max}\\
\textsc{geom} &= \mathit{pointinterval}
\end{align*}
  \end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/2-xmin_xmax_interval.pdf}
  \end{minipage}
\hfill\break

Here, the magic values $-1.96$ and $+1.96$ are the $(1-95\%/2) = 2.5\%$th and $(1+95\%/2) = 97.5\%$th quantiles of the standard Normal distribution, thus yielding a $97.5\% - 2.5\% = 95\%$ interval. This is generic in the sense that any interval can be represented, but unsatisfying in the sense that we seem to have lost some level of abstraction that was present when we were just thinking in terms of estimates and their variances. This approach also requires that the user knows how to make these calculations. Both \textit{ggplot2} (with \texttt{geom\_pointrange}) and \textit{Vega-lite} (with the \texttt{x} and \texttt{x2} channels supplied to \texttt{errorbar}) offer a variant of this solution for pre-calculated intervals. 

I will offer a different solution: to instead represent intervals as properties of a distribution, allowing us to neatly handle both the simple case of Gaussian error and more complex cases. Centering distributions---not standard deviations or intervals---in the specification of uncertainty will also allow us to build a richer set of uncertainty representations.

\section{Uncertainty visualization as distributional visualization}

\subsection{Intervals}

Imagine we represent an uncertain value generically as a distribution, or a random variable, $A$. Importantly, I do not consider this a \textit{probability} distribution necessarily: it could be a probability distribution, but it could also be a \textit{confidence} distribution, which is a frequentist generalization of sampling distributions and bootstrap distributions~\textbf{CITE}. Its defining characteristic will be that it has a cumulative distribution function (\textsc{cdf}), $F_A(x)$, which is:
\begin{itemize}
    \item For a probability distribution, $F_A(x) = \Pr(A \le x)$, the probability that $A$ is less than or equal to $x$.
    \item   For a confidence distribution, $F_A(x) = \gamma$  is the confidence $\gamma$ at which $x$ would be the upper limit of a one-sided $\gamma\%$ confidence interval, $[-\infty, x]$, for $A$. That sentence is pretty typical of convoluted frequentist definitions, so it might be easier to think in terms of the inverse: $F_A^{-1}(\gamma)$ yields the value $x$, which is the upper limit of a one-sided $\gamma\%$ confidence interval on $A$: $[-\infty,x]$. So $[-\infty, F_A^{-1}(0.95)]$ is a one-sided 95\% confidence interval on $A$.
\end{itemize}

For either representation, we may also be interested in other functions of the distribution. These include the derivative of the cumulative distribution function, i.e. the density function (or the mass function, if the distribution is discrete), $f_A(x)$, as well as the inverse of the \textsc{cdf} (also known as the quantile function), $F_A^{-1}(x)$. Given these functions, we can generate a variety of uncertainty representations, including but not limited to density plots and intervals.

For example, a median and $\gamma\%$ quantile interval could be defined generically on any distribution $A$ as follows:

\noindent
\begin{minipage}{.5\columnwidth}
\begin{align*}
\operatorname{median}(A) &\rightarrow x\\
F_A^{-1}\left(\frac{1 - \gamma}{2}\right) &\rightarrow x_\textsc{min}\\
F_A^{-1}\left(\frac{1 + \gamma}{2}\right) &\rightarrow x_\textsc{max}\\
\textsc{geom} &= \mathit{pointinterval}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-geom_pointinterval_quantiles.pdf}
  \end{minipage}
\hfill\break

If $A$ is a probability distribution, this is a Bayesian \textit{credible }interval, and if $A$ is a confidence distribution, this is a frequentist \textit{confidence} interval. This lets us abstract over the petty battles between this or that statistical camp and get to the meaningful business of visualizing uncertainty. This also allows us something not present in other attempts so far: to make it easy to specific multiple interval sizes, and to map interval size itself onto an aesthetic. For example, if we\footnote{Yes yes, I am using both ``I'' and ``we'' in this paper. ``I'' is me, and ``we'' is the conspiratorial ``we'': I'd like to hope you'll come with me on the journey of trying to sort out reasonable ways of visualizing uncertainty.} wanted to show two intervals, a 95\% and a 66\%, where the smaller interval is shown as a thicker line, we could write:

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
\operatorname{median}(A) &\rightarrow x\\
F_A^{-1}\left(\frac{1 - \gamma}{2}\right) &\rightarrow x_\textsc{min}\\
F_A^{-1}\left(\frac{1 + \gamma}{2}\right) &\rightarrow x_\textsc{max}\\
-\gamma &\rightarrow \mathit{linewidth}\\
\textsc{geom} &= \mathit{pointinterval}\\
\gamma &\in \{0.66, 0.95\}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-stat_pointinterval_linewidth.pdf}
  \end{minipage}
\hfill\break

This is a not uncommon approach that tries to avoid dichotomous thinking by showing multiple intervals of different masses. It also has the nice grammar-of-graphics-ish property of mapping the mass ($\gamma$) onto the width of the line, instead of creating two explicit, separate layers, each specifying a different interval---it makes the mass into \textit{data}.\footnote{I learned at least two useful things from a relational databases class in undergrad: (1) it's always better to put data into rows than into column names of tables---an insight that stems from database \textit{normal forms}~\textbf{CITE} (distinctions between which I have long forgotten) or what some statisticians call \textit{tidy data}~\textbf{CITE}; and (2) you are rarely at Google scale, so you're probably better off with a relational database with proper transactions than some dumb old key value store. The latter lesson my grad students refuse to learn until they build a webapp to collect data from 300 participants using some newfangled database they aren't the target users for, and end up with garbage. Kids these days, etc.} This also makes it easy to generalize to other visualizations, e.g. by modifying the previous specification to map mass onto \textit{color} instead of \textit{linewidth}:


\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
-\gamma &\rightarrow \mathit{color}\\
\textsc{geom} &= \mathit{pointinterval}\\
\gamma &\in \{0.50, 0.80, 0.95\}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-stat_pointinterval_color.pdf}
  \end{minipage}
\hfill\break


On the other hand, these are still a bit low-level: they require the user to know how to calculate interval endpoints from the quantile function. This also limits us specifically to quantile intervals, when other intervals types, such as highest-density intervals~\textbf{CITE} and shortest intervals~\textbf{CITE}, might be preferable. Thus, \textit{ggdist} also supplies a \textit{stat} version of \textit{pointinterval}, which bundles up some statistical calculations and default aesthetic mappings with the \textit{pointinterval} geometry. All \textit{stat}s in \textit{ggdist} support the $x_\textsc{dist}$ and $y_\textsc{dist}$ aesthetics, onto which objects that represent distributions can be mapped. They also allow the user to specify the type of point and interval used, and generate the corresponding values and mappings for $x$, $x_\textsc{min}$, $x_\textsc{max}$, and \textit{linewidth}. This changes the specification to something like:


\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
A &\rightarrow x_\textsc{dist}\\
\textsc{stat} &= \mathit{pointinterval}\\
\textsc{point} &= \mathit{median}\\
\textsc{interval} &= \textit{quantile interval}\\
\gamma &\in \{0.66, 0.95\}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-stat_pointinterval_A.pdf}
  \end{minipage}
\hfill\break

The representation of the distribution $A$ could be a sample-based representation, e.g. a bunch of draws from a Bayesian posterior or from a bootstrap distribution, or it could be an object representing a theoretical distribution in terms of its parameters, such as a Normal distribution with a defined mean and standard deviation. Point estimates and interval types can be defined by arbitrary functions of distributions, and predefined functions for mean, median, and mode, and quantile, highest-density, and shortest intervals are provided. This generalizes the $\{x, x_\textsc{sd}\}$ approach used by \textit{Vega-lite} to any distribution type while abstracting over the specifics of how point estimates and intervals are calculated.

In implementation, \textit{ggdist} allows distributions to be represented by numeric vectors (sample-based representation), objects from the \textit{distributional } R package (which supports theoretical distributions), and \texttt{rvar} objects from the \textit{posterior} R package (a sample-based representation that mimics numeric arrays in R). For example, if we re-create the $\{x, x_\textsc{sd}\}$ representation abstractly thus:

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
\operatorname{Normal}(a, \sigma_a) &\rightarrow x_\textsc{dist}\\
\textsc{stat} &= \mathit{pointinterval}\\
\textsc{point} &= \mathit{median}\\
\textsc{interval} &= \textit{quantile interval}\\
\gamma &\in \{0.66, 0.95\}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-stat_pointinterval_normal.pdf}
  \end{minipage}
\hfill\break

In \textit{ggdist}, using \texttt{distributional::dist\_normal}, the specification is quite similar:

\begin{centerverbatim}
ggplot(data) +
  aes(xdist = dist_normal(a, sigma_a)) +
  stat_pointinterval(
    point_interval = median_qi, 
    .width = c(.66, .95)
  )
\end{centerverbatim}

These happen to be the default values for \texttt{point\_interval} and \texttt{.width} ($\gamma$),\footnote{For historical reasons that have to do with a combination of a very long discussion with a bunch of people on the Stan forums~\textbf{CITE} and naming conventions in some R \textsc{api}s for fixed arguments to functions with variable argument lists~\textbf{CITE}, $\gamma$ in \textit{ggdist} is spelled \texttt{.width}. Reflecting on my past mistakes, a better name would be  \texttt{mass}.} so just \texttt{stat\_pointinterval()}  also works here. To demonstrate generalizing this approach, consider the very common need of placing uncertainty intervals on the results of a $t$-test, which can be derived from a $t_\nu(\mu, \sigma)$  distribution with $\nu$ degrees of freedom, scale $\mu$ (e.g. an estimated mean), and scale $\sigma$ (e.g. a standard error). Given these three numbers in a data frame, a visualization specification might be:

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
t_{\nu_a}(a, \sigma_a) &\rightarrow x_\textsc{dist}\\
\textsc{stat} &= \mathit{pointinterval}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-stat_pointinterval_student_t.pdf}
  \end{minipage}
\hfill\break


Which in \textit{ggdist} is:

\begin{centerverbatim}
ggplot(data) +
  aes(xdist = dist_student_t(nu_a, a, sigma_a)) +
  stat_pointinterval()
\end{centerverbatim}

\subsection{Ribbons}

Once we have \textit{pointinterval} representations, it is straightforward to develop uncertainty band representations by generalizing points to lines and intervals to ribbons---thus, \textit{lineribbon}. Imagine a regression that models car miles per gallon based on weight (the details of the function $g$ are not important):

\[
\log(\mathit{mpg}) \sim \operatorname{Normal}\left(g(\mathit{weight}), \sigma\right)
\]
Such a model could provide a predictive distribution for a car's miles per gallon conditional on its weight: $p(\mathit{mpg} \mid \mathit{weight})$, which we might want to plot alongside the raw data. If a \textit{lineribbon} is a geometry combining a line with an arbitrary number of uncertainty bands around it, abstractly, we want something like this:

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
\mathit{weight} &\rightarrow x\\
p(\mathit{mpg} \mid \mathit{weight}) &\rightarrow y_\textsc{dist}\\
\gamma &\rightarrow \mathit{fill}\\
\textsc{stat} &= \mathit{lineribbon}\\
\gamma &\in \{0.50, 0.80, 0.95\}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-lineribbon.pdf}
  \end{minipage}
\hfill\break


I include the raw data as a separate \textit{point} geometry layer as well, for comparison. Assuming \texttt{m}  is a Bayesian version of such a model fit using the \textit{brms} modeling package in R, and \texttt{preds} is a data frame of desired \textit{weight} values to predict on, we can add a column to \texttt{preds} that contains a random variable representation of $p(\mathit{mpg} \mid \mathit{weight})$ using \texttt{brms::posterior\_predict}~\textbf{CITE} and the \texttt{posterior::rvar}  data type~\textbf{CITE}. The latter is a data type I created\footnote{This was a slightly insane idea in itself, given the way that base R data types work. As R's classes work based on generic \textit{functions}, rather than classes with a well-defined set of methods, this requires tracking down all the various functions implemented to work with arrays and vectors in the language and providing implementations for them for random variable arrays. Indexing functions especially are annoying, as there are half a dozen different ways to index into R arrays, each with myriad corner cases. Anyway.} specifically to wrap large samples that represent distributions into objects that mimic R vectors and arrays, and which can be added to data frames:

\begin{centerverbatim}
preds = preds |> mutate(
  mpg_given_weight = rvar(posterior_predict(m, preds)
)
\end{centerverbatim}

Given this data frame, the equivalent of the abstract specification above is:

\begin{centerverbatim}
ggplot(preds) +
  aes(x = weight, ydist = mpg_given_weight) +
  stat_lineribbon()
\end{centerverbatim}

\texttt{stat\_lineribbon} defaults to \texttt{.width = c(.5, .8, .95)} and maps the resulting \texttt{.width} onto the \texttt{fill} aesthetic, so we do not need to specify \texttt{.width} or the \texttt{fill} mapping for this example. Once we have a multiple-ribbon geometry, it is easy to create other uncertainty visualization types, like gradient fan charts~\textbf{CITE}. For example, we could use a large number of intervals, say $k = 50$ or $100$, with masses between 0 and 1 (exclusive):

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
\gamma &\rightarrow \mathit{fill}\\
\textsc{stat} &= \mathit{lineribbon}\\
\gamma &\in \left\{\frac{i - 0.5}{k} \mathrel{}\middle|\mathrel{} i \in 1 \dots k \right\}\\
k &= 100
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-lineribbon_fan.pdf}
  \end{minipage}
\hfill\break


This set of $k$  $\gamma$ values is the same sequence generated by the \texttt{ppoints(k)} function in R, so we can pass \texttt{.width = ppoints(100)} to \texttt{stat\_lineribbon} to get a gradient fan chart with 100 intervals. This stems directly from the choice to make $\gamma$ into data that can be mapped onto aesthetics, and is one example of support for a chart type that was a happy accident of \textit{ggdist}'s design.

\subsection{Slabs}

Speaking of gradients, the obvious other direction to go for uncertainty---if we are to move beyond intervals---is density plots. Most grammar of graphics implementations have an implementation of density plots, but these are typically designed only for sample-based representations: they calculate a kernel density estimate (\textsc{kde}) from a sample and allow this to be visualized. Given \textit{ggdist}'s core abstraction of distributions, we can take this a step further, visualizing both sample-based representations and theoretical distributions.\footnote{\label{foot:jacobians}You may be tempted to say: but Matthew, of course you could easily pre-calculate densities from a theoretical density function and plot them. Unfortunately, once you add non-linear axis transformation into the mix, this is a recipe for silent errors caused by failing to adjust the density by the derivative of the transformation; an error \textit{ggdist} prevents. See \cref{sec:jacobians}.} 

However, stopping at \textit{just} densities seems the wrong level of abstraction:\footnote{See earlier footnote\textsuperscript{\ref{foot:errorbar}} about \textit{error bars} versus \textit{intervals}.} many useful uncertainty visualizations can be created through the whole suite of distributional functions: \textsc{cdf}s, densities, and quantiles. Thus, \textit{ggdist} instead has a notion of a \textit{slab} geometry, which has a \textit{thickness} onto which arbitrary functions of the distribution can be mapped. For example, imagine two \textit{group}s, \textit{a} and \textit{b}, each with uncertainty in its mean represented by a \textit{t}-distributed random variable $M \mid group$ with degrees of freedom $\nu$, mean $\mu$, and scale $\sigma$. We could specify a density plot of these distributions as:

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
t_\nu(\mu, \sigma) &\rightarrow x_\textsc{dist}\\
\textit{group} &\rightarrow y\\
f_M(x) &\rightarrow \textit{thickness}\\
\textsc{stat} &= \mathit{slab}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_density_two_groups.pdf}
  \end{minipage}
\hfill\break

Here, the \textit{slab}s have a subscale for \textit{thickness}, which is an orientation-aware aesthetic (if the distributions were mapped onto $y_\textsc{dist}$, \textit{thickness} would act as a width instead of height), with a fixed baseline: values of 0 on the thickness scale always correspond to the base of the slab. This makes it appropriate for both probability densities and $\textsc{cdf}$s, both of which have a natural 0 point. In the above example, the density $f_M(x)$ is mapped onto \textit{thickness}, creating a traditional density plot. Importantly, because the geometries use the same scale, both subscales have the same overall maximum \textit{thickness}, which ensures that the area under both densities is equal.\footnote{This default is, most of the time, what you want, as it preserves the property that each distribution integrates to 1. For exceptions to this rule, \textit{ggdist} provides a \texttt{normalize} option and a \texttt{scale\_thickness\_shared} function which allow finer control over how thickness scales are shared across groups, panels, and geometries.} 

This is the default output of \texttt{stat\_slab()} in \textit{ggdist}, which maps densities onto the \textit{thickness} aesthetic. If desired, the mapping $f_M(x) \rightarrow \textit{thickness}$ can be translated to code in one of two ways:

\begin{enumerate}
    \item \texttt{aes(thickness = after\_stat(pdf))}: This says to map the \texttt{pdf} \textit{computed variable} onto \textit{thickness}. Computed variables in \textit{ggplot2} are calculated by \textit{stat}s and made accessible in aesthetic mappings by wrapping the mapping specification in \texttt{after\_stat()}.
    \item \texttt{aes(thickness = !!p\_(x))}:  This uses a small domain-specific language for probability expressions I added to \textit{ggdist}, and is intended to more closely mimic a mapping like $p(x) \rightarrow thickness$ in code. The \texttt{!!} pseudo-operator comes from the \textit{rlang} meta-programming R package, and performs \textit{unquotation}~\textbf{CITE}: it inserts the expression returned by \texttt{p\_(x)}, which in this case is \texttt{after\_stat(pdf)}, into the \texttt{aes} call.\footnote{I added this syntax relatively recently, when \textit{ggplot2} deprecated \texttt{stat()}, which used to be a synonym for \texttt{after\_stat()}. This (1) made expressions with \texttt{after\_stat} needlessly verbose and (2) replaced a declarative verb, \texttt{stat}---which indicates the type of expression at play but not when it is computed---with a procedural verb, \texttt{after\_stat}. I feel that this name change violates the declarative foundations of \textit{ggplot2}, and the mini-\textsc{dsl} for probabilistic expressions in \textit{ggdist} is my small protest against it.}
\end{enumerate}

Slab geometries have several useful properties which help make them useful for creating a variety of uncertainty visualizations. One important property is that the \textit{alpha} (opacity), \textit{fill}, and outline \textit{color} aesthetics of the slabs can have data values mapped onto them at a sub-geometry level.\footnote{Just as an implementation note, this happens to be an incredible pain in the ass. Prior to R 4.1, the R graphics engine did not have proper gradient support, so this involved manually sub-dividing geometries and interpolating thickness values at cutpoints between contiguous blocks of color. Now, with proper gradient support in some R graphics output formats (e.g. \textsc{svg} and \textsc{pdf}), \textit{ggdist} can output high-quality color gradients, and as of this writing is one of the only R packages to do so (even base \textit{ggplot2} has not implemented it yet).} This functionality, combined with the ability to map arbitrary distribution functions, means we can easily recreate a bunch of visualizations from the literature. The obvious first example would be a color gradient plot~\textbf{CITE}, by mapping density onto \textit{alpha}:


\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
t_\nu(\mu, \sigma) &\rightarrow x_\textsc{dist}\\
\textit{group} &\rightarrow y\\
f_M(x) &\rightarrow \textit{alpha}\\
\textsc{stat} &= \mathit{slab}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_gradient_two_groups.pdf}
  \end{minipage}
\hfill\break

Which translates naturally to \texttt{aes(alpha = !!p\_(x))}. Fine fine, but let's get \textit{weird}. Back in 2014, the inimitable Michaels Correll and Gleicher wrote that error bars should be considered harmful~\cite{correll2014error}, and proposed instead a visualization which has a solid bar inside the 95\% interval and gradient tails that fade out beyond the interval, to emphasize the arbitrariness of the 95\% confidence level. We can use the \textsc{cdf} to construct a function with these properties, and map it to \textit{alpha}:

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
-max \left( \begin{matrix} \lvert 2 F_M(x) - 1 \rvert \\ 0.95 \end{matrix} \right) &\rightarrow \textit{alpha}\\
\textsc{stat} &= \mathit{slab}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_gradient_correll.pdf}
  \end{minipage}
\hfill\break

In \textit{ggdist} this function is either:
\begin{itemize}
    \item \texttt{-pmax(abs(2*after\_stat(cdf) - 1), .95)} or
    \item    \texttt{-pmax(abs(2*!!Pr\_(X <= x) - 1), .95)}, using the probability expression mini-\textsc{dsl}.
\end{itemize}

Thus, \textit{ggdist} can create this particularly weird uncertainty visualization without breaking the bonds of its core abstraction. But let's get \textit{weirder}: in 2021, Helske et al~\cite{helske2021can}, inspired by Correll and Gleicher, proposed combining this gradient tail with a violin plot. We can do that by adding back in the density-to-thickness mapping, and use the \textsc{side} parameter, which specifies if the slab should be drawn on the \textit{top} side (default), \textit{bottom} side, or \textit{both}. I'll also adjust the tails to fade outside the 85\% interval, since otherwise the fading is hard to see in the skinny tails of the violin:

 \noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
f_M(x) &\rightarrow \textit{thickness}\\
-max \left( \begin{matrix} \lvert 2 F_M(x) - 1 \rvert \\ 0.85 \end{matrix} \right) &\rightarrow \textit{alpha}\\
\textsc{stat} &= \mathit{slab}\\
\textsc{side} &= \mathit{both}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_violin_gradient.pdf}
  \end{minipage}
\hfill\break

Okay fine, but let's get \textit{even weirder}: Helske et al~\cite{helske2021can} further suggested using discrete intervals for the colors instead of a gradient, to aid discriminability. We can do that too! A key feature of the \textit{slab} stat is that it \textit{also} computes intervals, and for each point along the slab, retains a column indicating the mass ($\gamma$) of the smallest requested interval containing that point. This means that we can integrate intervals directly into the slab by mapping $\gamma$ onto \textit{alpha} or \textit{fill}:


 \noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
f_M(x) &\rightarrow \textit{thickness}\\
\gamma &\rightarrow \textit{fill}\\
\gamma &\in \{0.5, 0.8, 0.95, 1\}\\
\textsc{stat} &= \mathit{slab}\\
\textsc{side} &= \mathit{both}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_violin_interval.pdf}
  \end{minipage}
\hfill\break

Helske describes these violin-interval plots as ``more challenging to create''; \textit{ggdist} supports them naturally through a combination of its features not \textit{specifically} designed to create these plots. Another variation on violin plots is the \textit{raindrop plot} of Barrowman and Myers~\cite{barrowman2003raindrop}, which maps log-density instead of density onto \textit{thickness} inside a desired interval, say 95\%. Let's combine that with a \textit{pointinterval} (spec not shown):


 \noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
\arraycolsep=2.5pt \left.\begin{array}{ll} \log f_M(x) & \textrm{if } \gamma \le .95\\ \varnothing & \textrm{otherwise}\end{array}\right\} &\rightarrow \textit{thickness}\\
\textsc{stat} &= \mathit{slab}\\
\textsc{side} &= \mathit{both}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_raindrop.pdf}
  \end{minipage}
\hfill\break

The rationale here is that some distributional features, such as fat tails (kurtosis), can be easier to see in log-density than density~\cite{barrowman2003raindrop}. In \textit{ggdist}, the above spec is written:

\begin{centerverbatim}
aes(thickness = after_stat(ifelse(.width <= .95, log(pdf), NA))) +
stat_slab(side = "both", normalize = "groups")
\end{centerverbatim}

Where \texttt{normalize = "groups"} is needed to tell the slab to normalize thickness on a per-group basis, as log-density does not have a natural zero point.

Another interesting\footnote{\textsc{aka} weird.}\footnote{Yes, there are a lot of footnotes. No, I don't care.} uncertainty visualization from the literature is Haber and Wilkinson's~\cite{haber1982perceptual} \textit{fuzzygram}, i.e. a \textit{fuzzy histogram} (or more generally, a \textit{fuzzy bar chart}). This chart type has what I would call a compelling \textit{generative story}: an explanation you can give for how the uncertainty encoding in the chart arises based on some generative process. Imagine we want to add uncertainty to a bar chart, and we have a distribution for the uncertainty in the value encoded in each bar. Now say we sample a large number of semi-transparent bar charts from these distributions, and overlay them all on top of each other. As the number of charts approaches $\infty$ and the opacity of any given chart goes to 0, the stack of overlapping bars for each value will begin to resemble the complementary \textsc{cdf}, $1 - F(x)$, of that distribution.\footnote{Assuming additive blending and opacities that sum to 1.} That insight leads to the following encoding (with a \textit{pointinterval} overlaid for reference):

 \noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
1 - F_M(x) &\rightarrow \textit{alpha}\\
\textsc{stat} &= \mathit{slab}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_fuzzygram.pdf}
  \end{minipage}
\hfill\break

I would describe this chart type as theoretically interesting but interpretationally problematic. Anecdotally, without the overlaid reference point and interval, folks I show this to often do not correctly identify the mean of the distribution, believing the darker parts of the bar are more likely (a not unreasonable misinterpretation).\footnote{I don't know if this is an example of within-the-bar bias~\cite{newman2012bar} or not.} On the other hand, I do like the idea of \textit{generative stories} in uncertainty visualization; for example, I've used Plinko boards to depict uncertainty in election forecasts~\textbf{CITE}, which try to capitalize on a physical process---one that might intuitively\footnote{As this is an uncertainty vis paper, I am legally required to use ``intuitive'' at least once without defining it.} feel random to people---to depict uncertainty through a form of \textit{sedimentation}~\textbf{CITE}.

Another advantage to being able to map arbitrary data values onto slab aesthetics is that we can map the results of logical conditions onto those aesthetics. A common procedure in the Bayesian estimation community is to use regions of practical equivalence (\textsc{rope})~\cite{kruschke2018rejecting}. Say, for example, you are interested in whether an estimate is ``practically'' equal to 0; you would define a \textsc{rope} around 0 of $\pm$ some small effect size within which you would consider the value so close to 0 it is effectively indistinguishable from it. Then you can ask questions like, what is the probability the value is practically equivalent to 0? We can visualize this probability by highlighting the \textsc{rope} on a density:

\noindent
\begin{minipage}{.5\columnwidth}

\begin{align*}
f_M(x) &\rightarrow \textit{thickness}\\
\lvert x \rvert < 2 &\rightarrow \textit{fill}\\
\textsc{stat} &= \mathit{slab}
\end{align*}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_rope.pdf}
  \end{minipage}
\hfill\break

Here, $\Pr(\lvert x \rvert < 2)$---the probability the value is in a \textsc{rope} of $\pm2$---is directly encoded by the proportion of the area under the curve highlighted in blue. In \textit{ggplot2}, logical conditions can be mapped directly onto aesthetics, so in \textit{ggdist}, we can use \texttt{aes(fill = after\_stat(abs(x) < 2))} to create the above chart.

\subsection{Further examples}

The preceding sections attempt to give a taste for why \textit{ggdist} is structured the way it is, its underlying formalism, and how it extends the grammar of graphics in a principled way to support a variety of uncertainty visualizations. Yet this only scratches the surface of what it can do: the interested reader (hopefully you!) should check out the vignettes in the \href{https://mjskay.github.io/ggdist/}{\textit{ggdist} documentation}, which include a slew of examples not covered here.

\section{Reflections and lessons learned}

\textit{ggdist} has been under development in some form or another for about six--seven years, and over that time has evolved through user feedback and several waves of refactoring and rewriting. Through all that, I'd hope I've learned a few things that may be useful for others tackling uncertainty visualization to consider.

\subsection{Something about dist syntax}



\subsection{Uncertainty is difficult to factor out into data pre-processing steps}
\label{sec:jacobians}

As an earlier footnote alluded,\textsuperscript{\ref{foot:jacobians}}   we must be very careful when visualizing density functions. A na\"{i}ve implementation of \texttt{stat\_slab} might simply pass $x$ values through the density function of the distribution mapped to $x_\textsc{dist}$. However, if a non-linear scale transformation is applied, this will result in incorrect densities. This is because, for a random variable $Y = g(X)$, the density function $f_Y$ is:

\[
f_Y(y) = f_X\left(g^{-1}(y)\right) \cdot \left| {g^{-1}}'(y) \right|
\]
In other words, if we have a random variable $X$ that we transform via the function $g$ to get $Y$, we must adjust its density values by the factor $\lvert {g^{-1}}'(y) \rvert$, the absolute value of the derivative of the inverse of $g$. To make things concrete, consider a log-Normal variable $X$:

\[
X \sim \operatorname{log-Normal}(0, 1)
\]
We could plot this distribution in base \textit{ggplot2} by using \texttt{ggplot2::stat\_function} combined with R's built-in log-Normal density function, \texttt{dlnorm}:

\noindent
\begin{minipage}{.5\columnwidth}
\small
\begin{verbatim}
ggplot() +
  stat_function(
    fun = \(x) dlnorm(x, 0, 1)
  )
\end{verbatim}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_fuzzygram.pdf}
  \end{minipage}
\hfill\break


If we plot this on a log scale, we should hope to see Gaussian density with a median of $10^0 = 1$ (since medians are preserved under transformation). However, because \textit{ggplot2} has no way to know this function is a density (nor does it necessarily know the derivative of the scale function), the resulting density will be incorrect:

\noindent
\begin{minipage}{.5\columnwidth}
\small
\begin{verbatim}
ggplot() +
  stat_function(
    fun = \(x) dlnorm(x, 0, 1)
  ) +
  scale_x_log10()
\end{verbatim}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_fuzzygram.pdf}
  \end{minipage}
\hfill\break

Clearly the median does not divide the area into two regions with equal mass, as it should. I have no doubt this has lead to user errors in \textit{ggplot2}.\footnote{For example, issue \href{https://github.com/tidyverse/ggplot2/issues/4783}{#4783} in the \textit{ggplot2} repository asks why a theoretical density and samples from a distribution do not line up under transformation---a less attentive user might never have noticed.}. Fortunately, \textit{ggdist} does know about Jacobian adjustments. It uses a combination of symbolic and (when necessary) numeric differentiation to calculate derivatives of \textit{ggplot2} scale transformations to adjust densities.\footnote{Specifically, \textit{ggdist} applies the built-in \texttt{D} function to get the symbolic derivative of the expression defining the scale function and, if that fails, falls back to \texttt{numDeriv::jacobian}~\textbf{CITE}. In the future, if my pull request \href{https://github.com/r-lib/scales/pull/341}{#341} to the \textit{scales} package is accepted, \textit{ggdist} will be able to use automatic differentiation instead, which is more reliable.} As a result, we can visualize a log-transformed log-Normal density and get the correct result:


If we plot this on a log scale, we should hope to see Gaussian density with a median of $10^0 = 1$ (since medians are preserved under transformation). However, because \textit{ggplot2} has no way to know this function is a density (nor does it necessarily know the derivative of the scale function), the resulting density will be incorrect:

\noindent
\begin{minipage}{.5\columnwidth}
\small
\begin{verbatim}
ggplot() +
  aes(xdist = dist_lognormal(0,1)) +
  stat_slab() +
  scale_x_log10()
\end{verbatim}
\end{minipage}%
  \begin{minipage}{.4\columnwidth}
    \centering
    \includegraphics[width=1.2\columnwidth]{figs/3-slab_fuzzygram.pdf}
  \end{minipage}
\hfill\break

This emphasizes the need for uncertainty visualization systems to be \textit{scale-aware}: it is not possible to correctly implement uncertainty visualization purely as a data pre-processing step.

\section{Discussion}

\begin{itemize}
    \item something about GG giving insights into correctness, possible tasks
    \item facility for brainstorming
    \item beyond univariate
    \item 
\end{itemize}

\section*{Supplemental Materials}
\label{sec:supplemental_materials}

Refer to the instructions for this section (\cref{sec:supplement_inst}).
Below is an example you can follow that includes the actual supplemental material for this template:

All supplemental materials are available on OSF at \url{https://doi.org/10.17605/OSF.IO/2NBSG}, released under a CC BY 4.0 license.
In particular, they include (1) Excel files containing the data for and analyses for creating \cref{tab:vis_papers} and \cref{fig:vis_papers}, (2) figure images in multiple formats, and (3) a full version of this paper with all appendices.
Our other code is intellectual property of a corporation---Starbucks Research---and there is no feasible way to share it publicly.


\section*{Figure Credits}
\label{sec:figure_credits}

Refer to the instructions for this section (\cref{sec:figure_credits_inst}).
Here are the actual figure credits for this template:

\Cref{fig:teaser} image credit: Scott Miller / Special to the Vancouver Sun, January 22, 2009, page A6.

\Cref{fig:vis_papers} is a partial recreation of Fig.\ 1 from \cite{Isenberg:2017:VMC}, which is in the public domain.


%% if specified like this the section will be ommitted in review mode
\acknowledgments{%
  \textbf{For IEEE VIS}, this section may be included in the \textbf{2-page allotment for References, Figure Credits, and Acknowledgments}.
	
	The authors wish to thank A, B, and C.
  This work was supported in part by a grant from XYZ (\# 12345-67890).%
}


\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}
%\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}

\bibliography{template}


%% ^^^^^   FOR IEEE VIS, EVERYTHING HERE MAY BE INCLUDED IN THE    ^^^^^ %%
%% 2-PAGE ALLOTMENT FOR REFERENCES, FIGURE CREDITS, AND ACKNOWLEDGEMENTS %%

\appendix % You can use the `hideappendix` class option to skip everything after \appendix

\section{About Appendices}
Refer to \cref{sec:appendices_inst} for instructions regarding appendices.

\section{Troubleshooting}
\label{appendix:troubleshooting}

\subsection{ifpdf error}

If you receive compilation errors along the lines of \texttt{Package ifpdf Error: Name clash, \textbackslash ifpdf is already defined} then please add a new line \verb|\let\ifpdf\relax| right after the \verb|\documentclass[journal]{vgtc}| call.
Note that your error is due to packages you use that define \verb|\ifpdf| which is obsolete (the result is that \verb|\ifpdf| is defined twice); these packages should be changed to use \verb|ifpdf| package instead.


\subsection{\texttt{pdfendlink} error}

Occasionally (for some \LaTeX\ distributions) this hyper-linked bib\TeX\ style may lead to \textbf{compilation errors} (\texttt{pdfendlink ended up in different nesting level ...}) if a reference entry is broken across two pages (due to a bug in \verb|hyperref|).
In this case, make sure you have the latest version of the \verb|hyperref| package (i.e.\ update your \LaTeX\ installation/packages) or, alternatively, revert back to \verb|\bibliographystyle{abbrv-doi}| (at the expense of removing hyperlinks from the bibliography) and try \verb|\bibliographystyle{abbrv-doi-hyperref}| again after some more editing.

\end{document}

